{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df54e55b-1a2e-4b2b-925b-0157569f2a2d",
   "metadata": {},
   "source": [
    "Coding Assignment1 - Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5aae0e-a131-4547-bf39-2f511de450dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import unidecode\n",
    "import sys  \n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow_hub as hub\n",
    "import gensim.downloader as api\n",
    "import concurrent.futures\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d6de9d-b21a-4978-a869-b8322571736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/e.d.i.t.h/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/e.d.i.t.h/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2cf66-0a3b-48e1-87f0-80b07bc1f3e2",
   "metadata": {},
   "source": [
    "After importing the required libraries start reading the csv file dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accca9fe-17d2-4102-979b-81f442c1b497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'text',\n",
       " 'text_sentiment',\n",
       " 'username',\n",
       " 'hashtags',\n",
       " 'created_at',\n",
       " 'user followers count',\n",
       " 'replycount',\n",
       " 'retweetcount',\n",
       " 'likecount',\n",
       " 'quotecount',\n",
       " 'language',\n",
       " 'media',\n",
       " 'retweetedTweet',\n",
       " 'quotedtweet',\n",
       " 'inReplyToTweetId',\n",
       " 'inReplyToUser',\n",
       " 'mentionedUsers']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6717b-935f-4554-b049-e810cd1f1606",
   "metadata": {},
   "source": [
    "Dropping the unwanted columns and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b113c0-84e4-4396-84ea-e561afe018cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the #nyse #stockmarketcrash happen?</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan:\\n\\nif a company isn't a quality c...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are \"buying on dip\" will very soon b...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@rdrhwke i wish our so-called president were t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment\n",
       "0      when will the #nyse #stockmarketcrash happen?        Neutral\n",
       "1  aaj ka gyan:\\n\\nif a company isn't a quality c...       Negative\n",
       "2  the stock market needs to crash hard to make i...       Negative\n",
       "3  those who are \"buying on dip\" will very soon b...        Neutral\n",
       "4  @rdrhwke i wish our so-called president were t...       Positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['id','username', 'hashtags','created_at', 'user followers count', 'replycount','retweetcount','likecount', 'quotecount', 'language', 'media', 'retweetedTweet', 'quotedtweet', 'inReplyToTweetId', 'inReplyToUser', 'mentionedUsers'], axis=1)\n",
    "df['text'] = df['text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a378fb34-b134-4440-8151-e536500e4ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment\n",
       "0         when will the nyse stockmarketcrash happen        Neutral\n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative\n",
       "2  the stock market needs to crash hard to make i...       Negative\n",
       "3  those who are buying on dip will very soon bec...        Neutral\n",
       "4  rdrhwke i wish our socalled president were tra...       Positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0670d3-8e3c-49b3-ace2-508d649bcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_removal(text):\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45cb0a49-3989-4223-ac64-25d81d7a52d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment\n",
       "0         when will the nyse stockmarketcrash happen        Neutral\n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative\n",
       "2  the stock market needs to crash hard to make i...       Negative\n",
       "3  those who are buying on dip will very soon bec...        Neutral\n",
       "4  rdrhwke i wish our socalled president were tra...       Positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(basic_removal)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795f68e-3a83-46a5-a65b-791afb6cb866",
   "metadata": {},
   "source": [
    "Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3413d3-9ff8-480b-9f3e-d7df08345893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern=re.compile(\"[\"\n",
    "                             u\"\\U0001F600-\\U0001F64F\" #emoticons\n",
    "                             u\"\\U0001F300-\\U0001F5FF\" #symbols, pictograph\n",
    "                              u\"\\U0001F680-\\U0001F6FF\" #transport and map symbol\n",
    "                              u\"\\U0001F1E0-\\U0001F1FF\" # flags(IOS)\n",
    "                              u\"\\U00002702-\\U000027B0\"\n",
    "                              u\"\\U00002FC2-\\U0001F251\"\n",
    "                             \"]+\",flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b54a38-ad12-47c8-ae5f-1da05404e305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment  \\\n",
       "0         when will the nyse stockmarketcrash happen        Neutral   \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative   \n",
       "2  the stock market needs to crash hard to make i...       Negative   \n",
       "3  those who are buying on dip will very soon bec...        Neutral   \n",
       "4  rdrhwke i wish our socalled president were tra...       Positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0         when will the nyse stockmarketcrash happen  \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...  \n",
       "2  the stock market needs to crash hard to make i...  \n",
       "3  those who are buying on dip will very soon bec...  \n",
       "4  rdrhwke i wish our socalled president were tra...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(remove_emoji)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ed155-cb1b-4206-aa80-b7ab3a68d3f5",
   "metadata": {},
   "source": [
    "Removing words with accents like è --> e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a3f68f2-e77d-42b9-9c58-6bffa9a38f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(keyword):\n",
    "    cleaned = unidecode.unidecode(keyword)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50394e31-cac2-477a-840e-0013e27ff701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment  \\\n",
       "0         when will the nyse stockmarketcrash happen        Neutral   \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative   \n",
       "2  the stock market needs to crash hard to make i...       Negative   \n",
       "3  those who are buying on dip will very soon bec...        Neutral   \n",
       "4  rdrhwke i wish our socalled president were tra...       Positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0         when will the nyse stockmarketcrash happen  \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...  \n",
       "2  the stock market needs to crash hard to make i...  \n",
       "3  those who are buying on dip will very soon bec...  \n",
       "4  rdrhwke i wish our socalled president were tra...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(remove_accents)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380847f-1ee0-4b32-bc0d-2600fe6a0bc4",
   "metadata": {},
   "source": [
    "To expand words like wouldn't to wouldnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb7b24fc-5a1f-4810-8efa-e1aafd26953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_words(text):\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "        # using contractions.fix to expand the shortened words\n",
    "        expanded_words.append(contractions.fix(word))   \n",
    "           \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52cbee65-3bfe-4de7-a93a-5037f0b0cc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>aaj ka gyan if a company is not a quality comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paulkrugman paul you wrote the stockmarketcras...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>paulkrugman paul you wrote the stockmarketcras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>checkmatey stockmarketcrash  best explanation\\...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>checkmatey stockmarketcrash best explanation d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stockmarketcrash  best explanation\\ndamn mask ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>stockmarketcrash best explanation damn mask cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stocks for fathers day\\n\\naapl\\nko\\npep\\nhd\\n...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>stocks for fathers day aapl ko pep hd low swk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this may push us toward ww \\ngreatdepression s...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>this may push us toward ww greatdepression sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment  \\\n",
       "0         when will the nyse stockmarketcrash happen        Neutral   \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative   \n",
       "2  the stock market needs to crash hard to make i...       Negative   \n",
       "3  those who are buying on dip will very soon bec...        Neutral   \n",
       "4  rdrhwke i wish our socalled president were tra...       Positive   \n",
       "5  paulkrugman paul you wrote the stockmarketcras...       Negative   \n",
       "6  checkmatey stockmarketcrash  best explanation\\...       Negative   \n",
       "7  stockmarketcrash  best explanation\\ndamn mask ...       Negative   \n",
       "8   stocks for fathers day\\n\\naapl\\nko\\npep\\nhd\\n...        Neutral   \n",
       "9  this may push us toward ww \\ngreatdepression s...       Negative   \n",
       "\n",
       "                                          clean_text  \n",
       "0         when will the nyse stockmarketcrash happen  \n",
       "1  aaj ka gyan if a company is not a quality comp...  \n",
       "2  the stock market needs to crash hard to make i...  \n",
       "3  those who are buying on dip will very soon bec...  \n",
       "4  rdrhwke i wish our socalled president were tra...  \n",
       "5  paulkrugman paul you wrote the stockmarketcras...  \n",
       "6  checkmatey stockmarketcrash best explanation d...  \n",
       "7  stockmarketcrash best explanation damn mask cr...  \n",
       "8  stocks for fathers day aapl ko pep hd low swk ...  \n",
       "9  this may push us toward ww greatdepression sto...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(expand_words)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e7301-2875-461c-be6c-f498bfd52e88",
   "metadata": {},
   "source": [
    "Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a11992-1f54-4e07-bafe-9c488986c495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/e.d.i.t.h/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06ca3ed3-48f2-4471-ab3b-df28db3eaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_stop_words = [\"banknifty\",\"stocks\",\"rdrhwke\",\"stockmarket\",\"stock market\",\"tictoctick\"]\n",
    "    text = str(text).lower()\n",
    "    stop_words = stopwords.words('english')\n",
    "    # stop_words2 = stopwords.words('hinglish')\n",
    "    stop_words.extend(new_stop_words)\n",
    "    # stop_words.extend(stop_words2)\n",
    "    stop_words = set(stop_words)\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_text.append('')  # Replace stopword with empty string\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f458be17-ebd7-4e74-9b35-f98b184c0df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>nyse stockmarketcrash happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>aaj ka gyan   company    quality company   buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>stock market needs  crash hard  make  realist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>buying  dip   soon become  promoters   comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>wish  socalled president  transitory   mean ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment  \\\n",
       "0         when will the nyse stockmarketcrash happen        Neutral   \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative   \n",
       "2  the stock market needs to crash hard to make i...       Negative   \n",
       "3  those who are buying on dip will very soon bec...        Neutral   \n",
       "4  rdrhwke i wish our socalled president were tra...       Positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0                       nyse stockmarketcrash happen  \n",
       "1  aaj ka gyan   company    quality company   buy...  \n",
       "2   stock market needs  crash hard  make  realist...  \n",
       "3     buying  dip   soon become  promoters   comp...  \n",
       "4    wish  socalled president  transitory   mean ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26dbbd-f0f6-4227-b259-a97157250161",
   "metadata": {},
   "source": [
    "Tokenization and Lemmatization for converting the word to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bafff78-9e83-4349-9323-6b3c581d10ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when will the nyse stockmarketcrash happen</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>nyse stockmarketcrash happen</td>\n",
       "      <td>nyse stockmarketcrash happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaj ka gyan\\n\\nif a company isnt a quality com...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>aaj ka gyan   company    quality company   buy...</td>\n",
       "      <td>aaj ka gyan company quality company buy price ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stock market needs to crash hard to make i...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>stock market needs  crash hard  make  realist...</td>\n",
       "      <td>stock market need crash hard make realistic ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>those who are buying on dip will very soon bec...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>buying  dip   soon become  promoters   comp...</td>\n",
       "      <td>buy dip soon become promoter company stockmark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rdrhwke i wish our socalled president were tra...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>wish  socalled president  transitory   mean ...</td>\n",
       "      <td>wish socalle president transitory mean really ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_sentiment  \\\n",
       "0         when will the nyse stockmarketcrash happen        Neutral   \n",
       "1  aaj ka gyan\\n\\nif a company isnt a quality com...       Negative   \n",
       "2  the stock market needs to crash hard to make i...       Negative   \n",
       "3  those who are buying on dip will very soon bec...        Neutral   \n",
       "4  rdrhwke i wish our socalled president were tra...       Positive   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                       nyse stockmarketcrash happen   \n",
       "1  aaj ka gyan   company    quality company   buy...   \n",
       "2   stock market needs  crash hard  make  realist...   \n",
       "3     buying  dip   soon become  promoters   comp...   \n",
       "4    wish  socalled president  transitory   mean ...   \n",
       "\n",
       "                                    clean_text_lemma  \n",
       "0                       nyse stockmarketcrash happen  \n",
       "1  aaj ka gyan company quality company buy price ...  \n",
       "2  stock market need crash hard make realistic ca...  \n",
       "3  buy dip soon become promoter company stockmark...  \n",
       "4  wish socalle president transitory mean really ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    # return doc\n",
    "    doc = nlp(text)\n",
    "    # Return only the lemmas of tokens that are not spaces or punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df['clean_text_lemma'] = df['clean_text'].apply(tokenize_lemma)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230da5bd-b1fd-4e92-b6c0-739cb5df956e",
   "metadata": {},
   "source": [
    "Splitting of dataset into training and testing sets. Have done twice one for bonus part which is USE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8223b5c6-34c0-4b1a-80a9-5c5a5a1e2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text_lemma']\n",
    "y = df['text_sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce85e53c-3ce2-4673-8ffe-7cb83e52ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text_lemma']\n",
    "y = df['text_sentiment']\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(X, y, test_size=0.2, stratify=y, random_state=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e661ab77-fae5-4121-9db8-a94cf3937c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da448939-8805-40a4-96fe-4f1ce7fca482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "count_vec = CountVectorizer()\n",
    "X_train_counts = count_vec.fit_transform(X_train)\n",
    "X_test_counts = count_vec.transform(X_test)\n",
    "\n",
    "chi2_selector_bow = SelectKBest(chi2, k=100)\n",
    "X_train_counts = chi2_selector_bow.fit_transform(X_train_counts, y_train_encoded)\n",
    "X_test_counts = chi2_selector_bow.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee763d44-9160-4cf2-a94c-0e3a4e501a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a6567d4-340f-4a50-8f7c-6624db91790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_selector = SelectKBest(chi2, k=100)\n",
    "X_train_chi2 = chi2_selector.fit_transform(X_train_tfidf, y_train_encoded)\n",
    "X_test_chi2 = chi2_selector.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "921ad5f7-5100-4fdd-8282-43fc5627da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and test data into sentence vectors # Encode sentences using USE\n",
    "X_train_use = use_model(X_train_text)\n",
    "X_test_use = use_model(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63d9eee9-6c80-428c-8d7a-7fbfab06885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8e92c65-3262-41b7-b16d-ae5c72eafeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype='float32')\n",
    "    n_words = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            n_words += 1\n",
    "            feature_vector += model[word] \n",
    "    \n",
    "    if n_words > 0:\n",
    "        feature_vector /= n_words\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ee96a18-7f61-49f0-945c-85aebb895c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = w2v_model.vector_size\n",
    "def process_document(text):\n",
    "    words = word_tokenize(text)\n",
    "    return average_word_vectors(words, w2v_model, num_features)\n",
    "\n",
    "# Use concurrent.futures to parallelize the processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    X_train_w2v = list(executor.map(process_document, X_train))\n",
    "    X_test_w2v = list(executor.map(process_document, X_test))\n",
    "\n",
    "X_train_w2v = np.array(X_train_w2v)\n",
    "X_test_w2v = np.array(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccd7d604-b318-44c4-abe2-9dda19c95ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe\n",
    "glove_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b8519ce-f052-4564-8afe-741860918eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_glove = glove_model.vector_size\n",
    "X_train_glove = np.array([\n",
    "    average_word_vectors(word_tokenize(text), glove_model, num_features_glove)\n",
    "    for text in X_train\n",
    "])\n",
    "X_test_glove = np.array([\n",
    "    average_word_vectors(word_tokenize(text), glove_model, num_features_glove)\n",
    "    for text in X_test\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "738c00f0-e854-4010-8556-eb253e0bceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier - PRIMARY\n",
    "svm_clf = svm.SVC(kernel='linear', random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a813d6-323f-4149-b537-163832d52a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - SECONDARY\n",
    "logreg_clf = LogisticRegression(random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5413e287-d1ad-4918-8694-5820b24b5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdf54483-65ea-4410-a464-8efefc319395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Results:\n",
      "Results with Bag of Words:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.56      0.64      1981\n",
      "     Neutral       0.63      0.93      0.75      2300\n",
      "    Positive       0.83      0.65      0.73      2509\n",
      "\n",
      "    accuracy                           0.72      6790\n",
      "   macro avg       0.74      0.71      0.71      6790\n",
      "weighted avg       0.74      0.72      0.71      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1104  603  274]\n",
      " [ 107 2131   62]\n",
      " [ 259  625 1625]]\n",
      "Accuracy: 0.7158\n",
      "\n",
      "Results with TF-IDF and Chi-Square Feature Selection:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.56      0.65      1981\n",
      "     Neutral       0.65      0.93      0.77      2300\n",
      "    Positive       0.83      0.68      0.75      2509\n",
      "\n",
      "    accuracy                           0.73      6790\n",
      "   macro avg       0.75      0.72      0.72      6790\n",
      "weighted avg       0.75      0.73      0.72      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1115  575  291]\n",
      " [ 115 2137   48]\n",
      " [ 238  574 1697]]\n",
      "Accuracy: 0.7289\n",
      "\n",
      "Results with Word2Vec Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.65      0.66      1981\n",
      "     Neutral       0.71      0.71      0.71      2300\n",
      "    Positive       0.71      0.72      0.71      2509\n",
      "\n",
      "    accuracy                           0.69      6790\n",
      "   macro avg       0.69      0.69      0.69      6790\n",
      "weighted avg       0.69      0.69      0.69      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1289  315  377]\n",
      " [ 307 1630  363]\n",
      " [ 355  354 1800]]\n",
      "Accuracy: 0.6950\n",
      "\n",
      "Results with GloVe Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.60      0.60      0.60      1981\n",
      "     Neutral       0.65      0.59      0.62      2300\n",
      "    Positive       0.64      0.69      0.67      2509\n",
      "\n",
      "    accuracy                           0.63      6790\n",
      "   macro avg       0.63      0.63      0.63      6790\n",
      "weighted avg       0.63      0.63      0.63      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1182  356  443]\n",
      " [ 400 1360  540]\n",
      " [ 377  392 1740]]\n",
      "Accuracy: 0.6306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models with SVM\n",
    "print(\"SVM Classifier Results:\")\n",
    "\n",
    "print(\"Results with Bag of Words:\")\n",
    "accuracy_bow_svm = train_evaluate(svm_clf, X_train_counts, y_train, X_test_counts, y_test)\n",
    "print(f\"Accuracy: {accuracy_bow_svm:.4f}\\n\")\n",
    "\n",
    "print(\"Results with TF-IDF and Chi-Square Feature Selection:\")\n",
    "accuracy_tfidf_svm = train_evaluate(svm_clf, X_train_chi2, y_train, X_test_chi2, y_test)\n",
    "print(f\"Accuracy: {accuracy_tfidf_svm:.4f}\\n\")\n",
    "\n",
    "print(\"Results with Word2Vec Embeddings:\")\n",
    "accuracy_w2v_svm = train_evaluate(svm_clf, X_train_w2v, y_train, X_test_w2v, y_test)\n",
    "print(f\"Accuracy: {accuracy_w2v_svm:.4f}\\n\")\n",
    "\n",
    "print(\"Results with GloVe Embeddings:\")\n",
    "accuracy_glove_svm = train_evaluate(svm_clf, X_train_glove, y_train, X_test_glove, y_test)\n",
    "print(f\"Accuracy: {accuracy_glove_svm:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4544f019-0a80-4a2d-9509-2052d0a397ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with Universal Sentence Encoder Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.64      0.64      1981\n",
      "     Neutral       0.72      0.72      0.72      2300\n",
      "    Positive       0.67      0.68      0.67      2509\n",
      "\n",
      "    accuracy                           0.68      6790\n",
      "   macro avg       0.68      0.68      0.68      6790\n",
      "weighted avg       0.68      0.68      0.68      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1258  260  463]\n",
      " [ 249 1662  389]\n",
      " [ 430  381 1698]]\n",
      "Accuracy: 0.6801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate with USE sentence vectors\n",
    "print(\"Results with Universal Sentence Encoder Embeddings:\")\n",
    "accuracy_use = train_evaluate(svm_clf, X_train_use, y_train_text, X_test_use, y_test_text)\n",
    "print(f\"Accuracy: {accuracy_use:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae041ab9-d411-41f5-8a78-6809b6546dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with Logistic Regression:\n",
      "Results with Bag of Words:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.56      0.64      1981\n",
      "     Neutral       0.65      0.91      0.76      2300\n",
      "    Positive       0.80      0.67      0.73      2509\n",
      "\n",
      "    accuracy                           0.72      6790\n",
      "   macro avg       0.73      0.71      0.71      6790\n",
      "weighted avg       0.73      0.72      0.71      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1105  546  330]\n",
      " [ 125 2089   86]\n",
      " [ 255  572 1682]]\n",
      "Accuracy: 0.7181\n",
      "\n",
      "Results with TF-IDF and Chi-Square Feature Selection:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.57      0.65      1981\n",
      "     Neutral       0.67      0.91      0.77      2300\n",
      "    Positive       0.81      0.70      0.75      2509\n",
      "\n",
      "    accuracy                           0.73      6790\n",
      "   macro avg       0.75      0.73      0.72      6790\n",
      "weighted avg       0.75      0.73      0.73      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1125  526  330]\n",
      " [ 124 2095   81]\n",
      " [ 242  506 1761]]\n",
      "Accuracy: 0.7336\n",
      "\n",
      "Results with Word2Vec Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.63      0.65      1981\n",
      "     Neutral       0.69      0.71      0.70      2300\n",
      "    Positive       0.70      0.72      0.71      2509\n",
      "\n",
      "    accuracy                           0.69      6790\n",
      "   macro avg       0.69      0.69      0.69      6790\n",
      "weighted avg       0.69      0.69      0.69      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1248  338  395]\n",
      " [ 294 1634  372]\n",
      " [ 315  382 1812]]\n",
      "Accuracy: 0.6913\n",
      "\n",
      "Results with GloVe Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.61      0.56      0.58      1981\n",
      "     Neutral       0.63      0.61      0.62      2300\n",
      "    Positive       0.63      0.69      0.66      2509\n",
      "\n",
      "    accuracy                           0.62      6790\n",
      "   macro avg       0.62      0.62      0.62      6790\n",
      "weighted avg       0.62      0.62      0.62      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1112  389  480]\n",
      " [ 358 1405  537]\n",
      " [ 358  431 1720]]\n",
      "Accuracy: 0.6240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models with Logistic Regression\n",
    "print(\"Results with Logistic Regression:\")\n",
    "\n",
    "print(\"Results with Bag of Words:\")\n",
    "accuracy_bow_logreg = train_evaluate(logreg_clf, X_train_counts, y_train, X_test_counts, y_test)\n",
    "print(f\"Accuracy: {accuracy_bow_logreg:.4f}\\n\")\n",
    "\n",
    "print(\"Results with TF-IDF and Chi-Square Feature Selection:\")\n",
    "accuracy_tfidf_logreg = train_evaluate(logreg_clf, X_train_chi2, y_train, X_test_chi2, y_test)\n",
    "print(f\"Accuracy: {accuracy_tfidf_logreg:.4f}\\n\")\n",
    "\n",
    "print(\"Results with Word2Vec Embeddings:\")\n",
    "accuracy_w2v_logreg = train_evaluate(logreg_clf, X_train_w2v, y_train, X_test_w2v, y_test)\n",
    "print(f\"Accuracy: {accuracy_w2v_logreg:.4f}\\n\")\n",
    "\n",
    "print(\"Results with GloVe Embeddings:\")\n",
    "accuracy_glove_logreg = train_evaluate(logreg_clf, X_train_glove, y_train, X_test_glove, y_test)\n",
    "print(f\"Accuracy: {accuracy_glove_logreg:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "569c58e2-c7b9-4978-b603-afb70b5f47ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with Universal Sentence Encoder Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.63      0.64      1981\n",
      "     Neutral       0.72      0.72      0.72      2300\n",
      "    Positive       0.66      0.68      0.67      2509\n",
      "\n",
      "    accuracy                           0.68      6790\n",
      "   macro avg       0.68      0.68      0.68      6790\n",
      "weighted avg       0.68      0.68      0.68      6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1241  257  483]\n",
      " [ 251 1660  389]\n",
      " [ 427  382 1700]]\n",
      "Accuracy: 0.6776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate with USE sentence vectors with Logistic Regression\n",
    "print(\"Results with Universal Sentence Encoder Embeddings:\")\n",
    "accuracy_use = train_evaluate(logreg_clf, X_train_use, y_train_text, X_test_use, y_test_text)\n",
    "print(f\"Accuracy: {accuracy_use:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
